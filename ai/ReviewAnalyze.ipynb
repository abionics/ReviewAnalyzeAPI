{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7yPJfW7t6Rm8"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import string\n",
    "import tarfile\n",
    "import time\n",
    "import urllib\n",
    "from collections import Counter\n",
    "\n",
    "import en_core_web_sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as functional\n",
    "from google.colab import drive\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "H3a7tq2F_qHt"
   },
   "outputs": [],
   "source": [
    "DATASET_URL = 'http://hidra.lbd.dcc.ufmg.br/datasets/yelp_2015/original/yelp_review_full_csv.tar.gz'\n",
    "DATASET_ARCHIVE_NAME = 'dataset.tar.gz'\n",
    "DATASET_FOLDER_NAME = 'dataset'\n",
    "\n",
    "ENGLISH = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2SMmLf367pG",
    "outputId": "8d8d5e7f-4089-4728-8c4b-8826e8a1ae31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Save/load data from file to increase speed when rerun this notebook\n",
    "def save(data, filename: str = 'data.pkl'):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)\n",
    "\n",
    "\n",
    "def load(filename: str = 'data.pkl'):\n",
    "    with open(filename, 'rb') as file:\n",
    "        return pickle.load(file)\n",
    "\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "def gload(filename: str = 'data.pkl'):\n",
    "    with open(f'/content/gdrive/My Drive/{filename}', 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XVXTzvMe_n-I",
    "outputId": "e427a58a-88dc-43d0-9bd2-e744a17f92cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.83 s, sys: 1.52 s, total: 7.35 s\n",
      "Wall time: 36.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Download dataset\n",
    "def download_file(url: str, filename: str, chunk_size: int = 2 ** 15):\n",
    "    with requests.get(url, stream=True) as request:\n",
    "        request.raise_for_status()\n",
    "        with open(filename, 'wb') as file:\n",
    "            for chunk in request.iter_content(chunk_size=chunk_size): \n",
    "                file.write(chunk)\n",
    "\n",
    "\n",
    "def extract_archive(archive_name: str, folder_name: str):\n",
    "    with tarfile.open(archive_name) as tar:\n",
    "        tar.extractall(path=folder_name)\n",
    "\n",
    "\n",
    "download_file(DATASET_URL, DATASET_ARCHIVE_NAME)\n",
    "extract_archive(DATASET_ARCHIVE_NAME, DATASET_FOLDER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PdeguBMWBhlH",
    "outputId": "0d88a9bf-d6f7-4d9d-edf3-98e202597f21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 650000\n",
      "CPU times: user 3.79 s, sys: 420 ms, total: 4.21 s\n",
      "Wall time: 4.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Read dataset\n",
    "test = pd.read_csv(f'{DATASET_FOLDER_NAME}/yelp_review_full_csv/test.csv', header=None)\n",
    "train = pd.read_csv(f'{DATASET_FOLDER_NAME}/yelp_review_full_csv/train.csv', header=None)\n",
    "\n",
    "test = test.rename(columns={0: 'label', 1: 'review'})\n",
    "train = train.rename(columns={0: 'label', 1: 'review'})\n",
    "\n",
    "print(len(test), len(train))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IiumB99HDKcF",
    "outputId": "fbf12941-f962-463e-a4bf-63ec78b1766e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 70.9 ms, sys: 7.59 ms, total: 78.5 ms\n",
      "Wall time: 83 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Tokenize in order to clean text\n",
    "def tokenize(text: str) -> list:\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    words = regex.sub(' ', text.lower())\n",
    "    words = re.sub(r'\\s+', ' ', words.strip(), flags=re.UNICODE)\n",
    "    return [token.text for token in ENGLISH.tokenizer(words)]\n",
    "\n",
    "\n",
    "# counts = Counter()\n",
    "# for index, row in train.iterrows():\n",
    "#     if index % 100 == 0:\n",
    "#         percent = 100 * index // len(train)\n",
    "#         print(f'{percent}%')\n",
    "#     counts.update(tokenize(row['description']))\n",
    "# save(counts, 'counts.pkl')\n",
    "counts = load('counts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "HfsQgBNVHdk6"
   },
   "outputs": [],
   "source": [
    "# Check words with spaces (must be empty)\n",
    "for word in list(counts):\n",
    "    if ' ' in word:\n",
    "        print(word)\n",
    "        print(counts[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Tnx7cFUm4Ivj"
   },
   "outputs": [],
   "source": [
    "# Create vocabulary\n",
    "counts_most = counts.most_common(2000)\n",
    "word2vec = {'': 0, 'UNK': 1}\n",
    "words = ['', 'UNK']\n",
    "for word, freq in counts_most:\n",
    "    word2vec[word] = len(words)\n",
    "    words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mthJOlVr4m0_",
    "outputId": "a4603014-3566-4407-92f7-05e35084ec4a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.81 s, sys: 1.51 s, total: 7.31 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Encode reviews to array of int\n",
    "def encode_sentence(text: str, word2vec: dict, size: int = 150):\n",
    "    tokenized = tokenize(text)\n",
    "    encoded = np.zeros(size, dtype=int)\n",
    "    enc1 = np.array([word2vec.get(word, word2vec['UNK']) for word in tokenized])\n",
    "    length = min(size, len(enc1))\n",
    "    encoded[:length] = enc1[:length]\n",
    "    return encoded, length\n",
    "\n",
    "\n",
    "# train['encoded'] = train['review'].apply(lambda x: np.array(encode_sentence(x, word2vec), dtype=object))\n",
    "# save(train, 'train.pkl')\n",
    "train = gload('ReviewAnalyze/pickles/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74M19XY64zqT",
    "outputId": "2c5d2702-3a7e-4bda-ccce-fab297447805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 42.1 s, sys: 0 ns, total: 42.1 s\n",
      "Wall time: 42.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Find and drop reviews with zero word length\n",
    "indexes = []\n",
    "for index, row in train.iterrows():\n",
    "    if row['encoded'][0][0] == 0:\n",
    "        indexes.append(index)\n",
    "train.iloc[indexes].head()\n",
    "train.drop(train.index[indexes], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRaiG1Eg9vIj",
    "outputId": "415b09fd-b364-49fc-b11d-e518d242cd81"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 129987, 1: 129998, 2: 129998, 3: 129998, 4: 129992})"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform labels from 1..5 to 0..4\n",
    "zero_numbering = {1: 0, 2: 1, 3: 2, 4: 3, 5: 4}\n",
    "train['label'] = train['label'].apply(lambda x: zero_numbering[x])\n",
    "Counter(train['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "eSIU7jlk9wYt"
   },
   "outputs": [],
   "source": [
    "# Split into train and validation subsets\n",
    "x = list(train['encoded'])\n",
    "y = list(train['label'])\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "lnPMIhEuA3ZW"
   },
   "outputs": [],
   "source": [
    "# Dataset \n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, x: list, y: list):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.from_numpy(self.x[idx][0].astype(np.int32))\n",
    "        return tensor, self.y[idx], self.x[idx][1]\n",
    "\n",
    "\n",
    "train_ds = ReviewsDataset(x_train, y_train)\n",
    "valid_ds = ReviewsDataset(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HF20QUM8BB30",
    "outputId": "06ce9805-1136-49f3-cf63-9ddc216edcad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check cuda device (GPU)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "50RO71kyB2su"
   },
   "outputs": [],
   "source": [
    "# Create train and validation functions\n",
    "def train_model(model: torch.nn.Module, epochs: int = 10, lr: float = 0.001):\n",
    "    since = time.time()\n",
    "    ep_time = time.time()\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lr)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x, y, l in train_dataloader:\n",
    "            x = x.long().to(device)\n",
    "            y = y.long().to(device)\n",
    "            y_pred = model(x, l)\n",
    "            optimizer.zero_grad()\n",
    "            loss = functional.cross_entropy(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print('train loss %.3f' % (loss))\n",
    "            time_elapsed = time.time() - ep_time\n",
    "            ep_time = time.time()\n",
    "            # Time spent for train/eval\n",
    "            # print(f'Complete in {time_elapsed // 60}m {time_elapsed % 60}s')\n",
    "            sum_loss += loss.item() * y.shape[0]\n",
    "            total += y.shape[0]\n",
    "        val_loss, val_acc, val_rmse = validation_metrics(model, valid_dataloader)\n",
    "        print('train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f' % (sum_loss / total, val_loss, val_acc, val_rmse))\n",
    "\n",
    "\n",
    "def validation_metrics(model: torch.nn.Module, dataloader: DataLoader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    sum_rmse = 0.0\n",
    "    for x, y, l in dataloader:\n",
    "        x = x.long().to(device)\n",
    "        y = y.long().to(device)\n",
    "        y_hat = model(x, l)\n",
    "        loss = functional.cross_entropy(y_hat, y)\n",
    "        pred = torch.max(y_hat, 1)[1].cpu()\n",
    "        correct += (pred == y.cpu()).float().sum().cpu()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item() * y.shape[0]\n",
    "        sum_rmse += np.sqrt(mean_squared_error(pred, y.cpu().unsqueeze(-1))) * y.shape[0]\n",
    "    return sum_loss / total, correct / total, sum_rmse / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "AmUwSXZCC4X7"
   },
   "outputs": [],
   "source": [
    "# Create LSTM net class\n",
    "class CustomLSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 5)\n",
    "        # initialize the hidden state (see code below)\n",
    "        self.hidden_dim = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        \"\"\"At the start of training, we need to initialize a hidden state\n",
    "        there will be none because the hidden state is formed based on previously seen data\n",
    "        So, this function defines a hidden state with all zeroes and of a specified size\"\"\"\n",
    "        # The axes dimensions are (n_layers, batch_size, hidden_dim)\n",
    "        return torch.zeros(1, 1, self.hidden_dim).to(device), \\\n",
    "               torch.zeros(1, 1, self.hidden_dim).to(device)\n",
    "\n",
    "    def forward(self, x, s):\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n",
    "        out_pack, (ht, ct) = self.lstm(x_pack)\n",
    "        out = self.linear(ht[-1])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gHQq3MokD4ZQ"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "batch_size = 2000\n",
    "vocab_size = len(words)\n",
    "train_dataloader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_ds, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nrxDXstWD_dG",
    "outputId": "28311b48-3ef9-4954-a566-11f1517b9c2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomLSTM(\n",
       "  (embeddings): Embedding(2002, 100, padding_idx=0)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (lstm): LSTM(100, 100, batch_first=True)\n",
       "  (linear): Linear(in_features=100, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "model = CustomLSTM(vocab_size, 100, 100)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GfQ4csmIEDoH",
    "outputId": "29f758fb-7953-4b9e-af9c-4947b2d8d09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss 1.240, val loss 1.025, val accuracy 0.555, and val rmse 0.924\n",
      "train loss 0.991, val loss 0.956, val accuracy 0.583, and val rmse 0.896\n",
      "train loss 0.940, val loss 0.932, val accuracy 0.594, and val rmse 0.869\n",
      "train loss 0.916, val loss 0.912, val accuracy 0.602, and val rmse 0.853\n",
      "train loss 0.899, val loss 0.911, val accuracy 0.603, and val rmse 0.851\n",
      "train loss 0.886, val loss 0.898, val accuracy 0.609, and val rmse 0.833\n",
      "train loss 0.876, val loss 0.892, val accuracy 0.610, and val rmse 0.830\n",
      "train loss 0.867, val loss 0.895, val accuracy 0.610, and val rmse 0.801\n",
      "train loss 0.859, val loss 0.887, val accuracy 0.613, and val rmse 0.828\n",
      "train loss 0.853, val loss 0.890, val accuracy 0.613, and val rmse 0.824\n",
      "train loss 0.847, val loss 0.891, val accuracy 0.613, and val rmse 0.827\n",
      "train loss 0.843, val loss 0.888, val accuracy 0.616, and val rmse 0.813\n",
      "train loss 0.837, val loss 0.885, val accuracy 0.616, and val rmse 0.811\n",
      "train loss 0.833, val loss 0.887, val accuracy 0.616, and val rmse 0.809\n",
      "train loss 0.829, val loss 0.886, val accuracy 0.615, and val rmse 0.814\n",
      "train loss 0.825, val loss 0.887, val accuracy 0.615, and val rmse 0.817\n",
      "train loss 0.821, val loss 0.882, val accuracy 0.616, and val rmse 0.802\n",
      "train loss 0.817, val loss 0.884, val accuracy 0.616, and val rmse 0.804\n",
      "train loss 0.814, val loss 0.887, val accuracy 0.616, and val rmse 0.805\n",
      "train loss 0.812, val loss 0.886, val accuracy 0.616, and val rmse 0.812\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "train_model(model, epochs=20, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "65iwTXDqQwzD"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sat4OL8yVJqI",
    "outputId": "acfe3572-a2f1-411d-d622-2c2463f6a1c9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n01fAYBREP3v",
    "outputId": "91488d68-8b57-4583-96e0-1fbcbe4ad9ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 606 ms, sys: 62 ms, total: 668 ms\n",
      "Wall time: 725 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Encode test dataframe\n",
    "# test['encoded'] = test['review'].apply(lambda x: np.array(encode_sentence(x, word2vec), dtype=object))\n",
    "# save(test, 'test.pkl')\n",
    "test = gload('ReviewAnalyze/pickles/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "S_Dbii62Ew9e"
   },
   "outputs": [],
   "source": [
    "indexes = []\n",
    "for index, row in test.iterrows():\n",
    "    if row['encoded'][0][0] == 0:\n",
    "        indexes.append(index)\n",
    "test.drop(test.index[indexes], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "sDkO690qE1Sp"
   },
   "outputs": [],
   "source": [
    "test['label'] = test['label'].apply(lambda x: zero_numbering[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "H4qVusCBFAeM"
   },
   "outputs": [],
   "source": [
    "# Create dataloader for test dataset\n",
    "x_test = list(test['encoded'])\n",
    "y_test = list(test['label'])\n",
    "test_dataset = ReviewsDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jD_QHjD_FGWL",
    "outputId": "98eae32c-cb57-41ba-c6d1-63bfb3ec3170"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss 0.885, test accuracy 0.615, and test rmse 0.811\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on test dataset\n",
    "test_loss, test_acc, test_rmse = validation_metrics(model, test_dataloader)\n",
    "print('test loss %.3f, test accuracy %.3f, and test rmse %.3f' % (test_loss, test_acc, test_rmse))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ReviewAnalyze.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
